# ğŸ“œ Amazon S3 Versioning  

## ğŸ§© Definition  
**S3 Versioning** is a feature that allows **multiple versions of the same object** to exist within a bucket.  
It is useful for **retrieving previous versions**, **recovering accidentally overwritten files**, or **restoring deleted objects**.  

---

## âš™ï¸ Features and Behavior  

### ğŸ› ï¸ Management and States  
- Versioning is automatically **managed** when objects are **overwritten** or **deleted** in a version-enabled bucket.  
- Buckets can have three states:  
  - **Unversioned (default)** â€“ no versioning applied.  
  - **Versioning-enabled** â€“ new versions are created automatically.  
  - **Versioning-suspended** â€“ versioning is paused; previous versions remain.  
- âš ï¸ **Note:** Once enabled, versioning **cannot be fully disabled**, only **suspended**.  

### âœ… Enabling Versioning  
- Can be enabled **during bucket creation** via the **Configure Options** step.  
- For **existing buckets**, it can be enabled via the **Properties** section in the **AWS Management Console**. 

### â¸ï¸ Suspended Versioning  
- **Suspended Versioning** refers to a state where versioning on a bucket is **not actively creating new versions**, but all **existing versions up to the point of suspension** are retained.  
- When versioning is **suspended**, **no new Version IDs** are assigned; changes to objects **overwrite the current version**.  
- This is different from **disabling versioning**, as once versioning is enabled, it **cannot be completely turned off**, only suspended.  
- ğŸ’° Suspending versioning can help **manage storage costs** by preventing the accumulation of multiple versions of objects.  
- Despite being suspended, **all previous versions remain accessible** and can be restored if needed.  

### ğŸ†” Version IDs  
- Each object receives a **Version ID** once versioning is enabled.  
- **New objects** uploaded get a **new Version ID**.  
- **Existing objects** have a `'null'` Version ID until modified or deleted.  

### ğŸ—‘ï¸ Deleting Objects  
- Deleting an object in a **version-enabled bucket** creates a **delete marker**, making the object appear deleted.  
- **Previous versions** remain accessible.  
- To **permanently delete** an object, delete the **specific version** using its **Version ID**.  

### ğŸ’¸ Cost Implications  
- Versioning **increases storage costs** because **multiple versions** of objects are stored.  
- Using **Lifecycle rules** or **Intelligent-Tiering** can help **manage costs**.  

### ğŸ”„ Lifecycle Management  
- Lifecycle configurations can:  
  - **Transition data** to lower-cost storage classes.  
  - **Delete noncurrent versions**.  
  - Clean up **incomplete multipart uploads**.  
- Helps **reduce storage expenses** while keeping versioned data manageable. 

---

### ğŸ§  Analogy: S3 Versioning  

Imagine you're working on a **document**, like an essay or report, on your computer.  

- Each time you make significant changes, you **save a new version** with a slightly different name, like `"Essay_v1"`, `"Essay_v2"`, and so on.  
- This allows you to **go back to an earlier version** if you preferred it or accidentally deleted a crucial part.  

In **S3 Versioning**, it works similarly:  

- When **versioning is enabled** on a bucket, every time an object is **modified or updated**, it is saved as a **new version** instead of overwriting the existing object.  
- You can **access, retrieve, or restore any previous version** of the object.  
- This helps **prevent data loss** and ensures you can always **return to a prior state** of your data if needed.  

---

## ğŸŒ Amazon S3 Static Website Hosting  

### ğŸ§© Definition  
**Static website hosting** on Amazon S3 allows you to **serve static content** such as HTML, CSS, and JavaScript **without server-side scripting**.  
This is useful for hosting simple websites, landing pages, or single-page applications.  

---

### âš™ï¸ Enabling Static Website Hosting  
- Navigate to the **Properties** page of your S3 bucket.  
- Select the **Static Website Hosting** tile. By default, this feature is **disabled**.  
- Three options are available:  
  1. **Use this bucket to host a website** â€“ requires specifying:  
     - **Index document**: the default homepage (e.g., `index.html`)  
     - **Error document**: displayed when an error occurs (e.g., `error.html`)  
  2. **Redirect requests** â€“ optional, for sending requests to another bucket or domain.  
  3. **Disabled** â€“ the default state.  

---

### ğŸ“œ Bucket Policy  

A **bucket policy** is a type of **resource-based policy** in AWS that is directly attached to an **S3 bucket** to manage **access permissions**.  

- Written in **JSON format**, it specifies **who (principal)** can perform **specific actions** on the bucket and its objects.  
- Unlike **identity-based policies** attached to IAM users, groups, or roles, **bucket policies are associated with the resource itself**.  
- Can be used to **grant cross-account access** without creating or assuming IAM roles.  
- Provides **granular control**, including conditions such as requiring **Multi-Factor Authentication (MFA)** for certain actions.  
- Can be **larger than IAM policies**, with a **maximum size of 20 KB**.  
- Can be used **alongside IAM policies and ACLs** for comprehensive access control.  
- AWS offers tools like the **AWS Policy Generator** to help create bucket policies.  
- By default, **new buckets have no bucket policy attached**.  
- Crucial for **maintaining security**, especially to **prevent unauthorized public access** to sensitive data stored in S3 buckets.  

---

### ğŸ”‘ Access Control Lists (ACLs)  

**Access Control Lists (ACLs)** are a method of controlling access to resources, such as **S3 buckets and objects**, by specifying **permissions for different groups or AWS accounts**.  

- ACLs allow setting **permissions at both the bucket and object levels**, but they are **less granular** compared to IAM and bucket policies.  
- They **do not support implicit deny or conditional elements**, unlike identity-based access controls.  
- Permissions are based on **predefined S3 groups**, including:  
  - **Bucket owner**  
  - **Everyone (public access)**  
  - **Authenticated users**  
  - **S3 log delivery group**  
- **Bucket-level permissions** include:  
  - List  
  - Write  
  - Bucket ACL Read  
  - Bucket ACL Write  
- **Object-level permissions** include:  
  - Read object  
  - Read object permissions  
  - Write object permissions  
- ACLs can be used **alongside IAM policies and bucket policies**, with **all policies evaluated together** to determine access.  
- By default, **access is denied unless explicitly allowed** by a policy or ACL.  
- Follows the principle of **least privilege** to ensure security. 

---

### ğŸŒ Accessing the Website  
- The website is accessed via a **region-specific endpoint URL**.  
- **HTTPS is not supported** for the S3 endpoint.  
- The bucket must be **publicly accessible**:  
  - Adjust permissions by **unchecking "Block all public access"** and confirming changes.  
  - Add a **bucket policy** to allow **public read access** using the `s3:GetObject` action.  

---

### ğŸš« Blocking Public Access  

Blocking public access is crucial to **prevent security breaches**, as leaving S3 buckets unprotected has previously exposed sensitive information.  

- AWS provides improved **security measures** to prevent unauthorized public access.  
- By default, **new S3 buckets block all public access**, which must be actively changed to allow public access.  
- Public access settings can block access through:  
  - **New or existing access controllers**  
  - **Public bucket policies**  
  - **Cross-account access**  
- To make a bucket **publicly accessible**, users must:  
  1. **Edit permissions** to unblock public access  
  2. **Confirm changes**  
  3. **Add a bucket policy** to allow public read access to objects  
- Required when configuring **S3 buckets for static website hosting**, including:  
  - **Index document**  
  - **Error document**  
  - Optional **redirection rules**  
- AWS provides **warnings and confirmations** when changing public access settings to highlight **potential security risks**.  
- The **"Block all public access"** setting **overrides other permissions**, ensuring that public access is removed when enabled.  

---

### ğŸ”„ Optional Features  
- **Redirection rules** can be configured using XML for advanced redirect scenarios.  
- A **custom domain name** can be used (outside the scope of this guide).  

---

### âœ… Testing  
- Once configured, test the website using the **provided endpoint URL**.  
- Ensure the **index and error documents** display correctly.  

---

### ğŸ§  Analogy: S3 Static Website Hosting  

Imagine you have a collection of your **favorite storybooks** that you want to **share with your friends**.  

- You set up a **small library in your front yard** where friends can come and read the books.  
- In this analogy:  
  - Your **storybooks** represent the **static content** of your website (HTML, CSS, JavaScript files).  
  - The **library** is the **S3 bucket** hosting your static website.  
- Just as you **organize and display your books** for easy access, you **configure your S3 bucket** to host your website.  
- The **Static Website Hosting** feature of S3 is like telling the world:  
  - `"Here's where you can come to read my storybooks"`,  
  allowing anyone with the **URL (library address)** to **view your website's content**. 

---

## ğŸ“ Amazon S3 Object-Level Logging  

### ğŸ§© Definition  
**Object-level logging** in Amazon S3 tracks **activities on individual objects** using **AWS CloudTrail**.  
It records API calls such as **GetObject**, **PutObject**, and **DeleteObject**, providing detailed visibility into **who accessed or modified objects**.  

---

---

## ğŸ“¥ GetObject Operation  
- Retrieves data from a storage service, such as Amazon S3.  
- Accesses a specific object in a bucket by specifying the **object key**.  
- Allows users to **download the object data** along with its **metadata**.  
- Commonly used to fetch **files, images, or other stored data** for processing or display.  
- Supports **partial retrieval**, like downloading a specific byte range.  
- Subject to **permissions and authentication** to ensure security.  

---

## ğŸ“¤ PutObject Operation  
- Uploads or stores an object (e.g., file or data) into a specified bucket.  
- Requires specifying the **bucket name**, **object key**, and **data** to upload.  
- Additional parameters can include **metadata**, **permissions**, and **storage class**.  
- Essential for **managing and storing data** in cloud storage efficiently.  

---

### âš™ï¸ How Object-Level Logging Works  
- **AWS CloudTrail** records and tracks **all AWS API requests**, capturing them as **events** stored in **log files on S3**.  
- Each API call is recorded with metadata, including:  
  - **Caller identity**  
  - **Timestamp**  
  - **Source IP address**  
- **S3 data events** can be configured in two ways:  
  1. Through the **AWS CloudTrail console**  
  2. At the **bucket level** via the **Properties tab** in S3  

---

### ğŸ”§ Configuration Steps  
1. **Select an existing CloudTrail trail** in the **same region**.  
2. Choose the **type of events** to capture:  
   - **Read events**  
   - **Write events**  
   - **Both Read and Write**  
3. AWS CloudTrail will then **capture S3 data events** for the bucket, providing **detailed insights** into object access and modifications.  

---

## ğŸ§  Analogy: S3 Object-Level Logging  

Imagine you have a **library full of books** (your S3 buckets), and each book represents a **piece of data** stored in your bucket.  

- **Object-Level Logging** is like having a **dedicated librarian** (AWS CloudTrail) who keeps a **detailed record** every time someone:  
  - **Checks out a book** ğŸ“–  
  - **Returns a book** ğŸ”„  
  - **Adds a new book** â•  
- The librarian notes **who did it, when it was done, and from where**.  
- If you ever need to know the **history of a specific book** or who has been accessing your library, you can **check the librarian's records**.  
- This ensures there is always a **clear trail of interactions** with your data, just like **CloudTrail tracks interactions with your S3 objects**.  

---

## ğŸ”’ Amazon S3 Object Lock  

### ğŸ§© Definition  
**Object Lock** is an advanced S3 bucket feature used to meet **compliance requirements** like **WORM (Write Once Read Many)**.  
It **protects objects from deletion or modification**, ensuring data immutability for regulatory or business purposes.  

---

### âš™ï¸ Key Features  
- Can only be **enabled during bucket creation** and requires **versioning** to be enabled first.  
- Once enabled, **Object Lock cannot be disabled**.  
- Offers **two retention modes**:  
  - **Governance Mode** ğŸ›¡ï¸: Users with specific permissions can **delete or modify objects** during the retention period.  
  - **Compliance Mode** âš–ï¸: **No user**, including the AWS root account, can **delete or modify objects** during the retention period.  
- **Retention periods** are set in **days**, specifying how long objects are **protected from deletion**.  
- Object Lock can be applied at the **bucket level** or on a **per-object basis**.  
- **Legal holds** can be applied to object versions, preventing deletion **without an expiration date** and can coexist with retention periods.  

---

### ğŸ§  Analogy: S3 Object Lock  

Imagine you have a **safe** where you keep **valuable items**.  

- Once you put something inside and **lock it**, you can:  
  - Set a **timer** that only allows it to be opened after a certain period. â³  
  - Decide the safe can **never be opened again**, protecting the contents **forever** ğŸ”’.  
- This is similar to the **Object Lock** feature in an S3 bucket.  
  - Enabling Object Lock is like putting your **digital objects** (files) into a **virtual safe**.  
  - You can set rules to **prevent deletion or alteration** for a set period or **indefinitely**, ensuring objects remain **untouched and secure**.  
- Object Lock is particularly useful for **compliance requirements**, ensuring data is **preserved in its original state** for the required duration, much like keeping **valuable documents in a safe** until the timer expires.  

---

## ğŸ·ï¸ Amazon S3 Cost Allocation Tags  

### ğŸ§© Definition  
**Cost allocation tags** for Amazon S3 are **key-value pairs** assigned at the **bucket level** to help **categorize and organize resources**.  
They are particularly useful for **managing budgets** across **different business units** and **departments**.  

---

### âš™ï¸ Key Features  
- Tags can indicate **projects** or **environments**, such as:  
  - `Environment: Production` ğŸ’¼  
  - `Environment: Test` ğŸ§ª  
  - `Project: CloudAcademy` â˜ï¸  
- Tags are **applicable across all AWS services**.  
- Use **AWS Billing and Cost Managementâ€™s Cost Explorer** to **report on these tags** and **track project-specific costs**.  

---

### ğŸ› ï¸ How to Add S3 Cost Allocation Tags  
1. **Select the bucket** you want to tag.  
2. Go to the **Properties tab**.  
3. **Configure the tags** by adding key-value pairs.  
4. **Save** the changes.  
5. **Activate the tags** in **AWS Billing** to ensure they appear in reports. ğŸ’°  

---

### ğŸ§  Analogy: S3 Cost Allocation Tags  

Imagine you have a **large library filled with thousands of books** ğŸ“š.  

- The books are stored on various shelves and sections, but **thereâ€™s no clear system** to categorize them by **genre, author, or publication date**.  
- You start placing **colored stickers** on the books' spines:  
  - Red for **Fiction** â¤ï¸  
  - Blue for **Non-Fiction** ğŸ’™  
  - Yellow for **Science Fiction** ğŸ’›  
- Over time, you also add stickers for **authors** and **publication years**.  

In this analogy:  
- The **library** = your **AWS environment** ğŸ¢  
- The **books** = your **S3 buckets** ğŸ“¦  
- The **colored stickers** = your **S3 tags** ğŸ·ï¸  

Just as the stickers help you **quickly identify and categorize books**, S3 tags help you **categorize, manage, and understand your AWS resources**.  

- Example: `"Project: Alpha"` or `"Environment: Production"`  
- Tags make it easier to **organize, search, and manage resources**, and also help with **tracking costs, usage, and security**, much like organizing a library simplifies finding specific books.  

---

## ğŸŒ Amazon CloudFront Overview  

### ğŸ§© Purpose and Functionality  
- **Amazon CloudFront** is a **content delivery network (CDN)** designed to distribute content with **low latency** and **high speed** âš¡  
- Caches content **closer to users** through a **global network of edge locations** ğŸŒ  
- Supports both **static and dynamic content**, improving performance by **reducing latency** and **origin server load**  

---

### âš¡ Caching and Distribution  
- CloudFront caches content at multiple layers:  
  - **Edge locations** ğŸ¢  
  - **Regional edge caches** ğŸŒ  
  - **AWS Origin Shield** ğŸ›¡ï¸  
- This **multi-layer caching** improves cache hit ratios and reduces the number of requests sent to origin servers  

---

### ğŸ› ï¸ Integration and Configuration  
- Integrates with **Amazon S3**, **EC2**, **Lambda**, and other AWS services  
- Users create a **CloudFront distribution**, specifying:  
  - **Origins**  
  - **Protocols**  
  - **Cache behaviors**  
  - **Security settings**  
- Supports **custom domain names** and **SSL certificates** for secure content delivery ğŸ”’  

---

### ğŸ”’ Security Features  
- Encrypted **SSDs** ğŸ’¾  
- **Signed URLs** and **cookies**  
- **AWS WAF** for web ACLs  
- **Geo-restrictions** ğŸŒ  
- Integrates with **AWS IAM** for administrative control  
- Monitored via **Amazon CloudWatch** and **AWS CloudTrail** ğŸ“Š 

---

## âš¡ Amazon S3 Transfer Acceleration  

### ğŸ§© Definition  
**Transfer Acceleration** is a feature that **speeds up data transfers** to and from Amazon S3, especially over **long distances** ğŸŒ.  
It leverages **Amazon CloudFront** (CDN) to route data through **AWS edge locations** strategically placed around the world.  

---

### âš™ï¸ How It Works  
- When enabled at the **bucket level**:  
  - Data transfer requests are routed through a **CloudFront Edge Location** ğŸ¢  
  - Then sent via an **optimized AWS network path** to S3 for faster transfers âš¡  
- Only works with **DNS-compliant bucket names** (no periods).  
- Requests must use the **transfer acceleration endpoint**.  

---

### ğŸ’° Costs  
- There is a **cost per GB** for using Transfer Acceleration, based on the **edge location** used.  
- **Standard data transfer into S3 from the internet is free**, so charges are only for accelerated transfers.  

---

### ğŸ› ï¸ How to Enable  
1. Select your **S3 bucket** in the AWS Management Console.  
2. Go to **Properties**.  
3. Choose the **Transfer Acceleration** option and **enable it**. âœ…  

---

### âš ï¸ Limitations  
- Some S3 operations are **not supported**:  
  - `GET Service` (list buckets)  
  - `PUT Bucket` (create bucket)  
  - `DELETE Bucket`  
  - Cross-region copies using `PUT Object - Copy` âŒ  

---

### ğŸ§  Analogy: S3 Transfer Acceleration  

Imagine you're in a **big city** trying to send a **package** to a friend in another country. ğŸ“¦  

- Normally, you'd take your package to the **local post office**, and it would **hop from one postal center to another** until it reaches its destination. This is like **uploading data to S3 without Transfer Acceleration**.  

- With **Transfer Acceleration**, the post office offers a **special service**:  
  - Your package is first sent to a **high-speed transport hub** in the city ğŸš€  
  - From there, it travels on a **direct, high-speed connection** to your friend's country, **bypassing slower, standard postal routes**  
  - Once it arrives, it's handed off to the **local delivery service** for final delivery ğŸ   

- Similarly, **Transfer Acceleration** uses **Amazon CloudFrontâ€™s globally distributed edge locations** as the **high-speed hubs**.  
  - Data is first sent to an **edge location**  
  - Then travels over **Amazonâ€™s optimized network path** to reach S3 much faster than over standard internet routes ğŸŒ 

---

## ğŸ’¸ Amazon S3 Requester Pays  

### ğŸ§© Definition  
**Requester Pays** is an S3 bucket feature that **shifts the cost of data requests and transfer** to the person or service accessing the data, while the **bucket owner continues to pay for storage costs**.  
This is useful for scenarios where **large datasets** are shared publicly or with multiple users, and the owner does not want to bear the cost of frequent downloads.  

---

### âš™ï¸ How It Works  
- **All access must be authenticated**; anonymous requests are **not allowed** âŒ  
- Enable the feature via the **bucket Properties** â†’ **Requester Pays** â†’ **Save changes** âœ…  
- Requests must include the **`x-amz-request-payer`** parameter in the request header to acknowledge the cost âš¡  

---

### ğŸ’° Cost Implications  
- **Bucket owner** pays for **storage only**  
- **Requesters** pay for **data transfer** and **request costs**  
- Helps distribute costs fairly when sharing **large or frequently accessed datasets**  

---

### ğŸ§  Analogy: Requester Pays  
Imagine you're at a **coffee shop** â˜• where you usually pay for your own coffee (data transfer and requests).  

- With the **Requester Pays** model, it's like asking a **friend (the requester)** to pick up your coffee order.  
- Your friend agrees and **covers the cost of the coffee being handed over**, while you (the bucket owner) still pay for the **coffee beans used to make the coffee** (storage costs).  
- To ensure the coffee shop knows your friend agreed, they have to say, "**I'm paying for this**" (include `x-amz-request-payer` in the header) when placing the order.  

---